# 达观杯之文本分类任务解析与代码使用
## 文本分类任务解析
### 文本分类任务基本框架
文本->特征工程->分类器->类别  
&emsp;&emsp;假设现在有一段文本需要对其进行分类，先对一段原始文本进行特征工程，将文本转换至相应的特征向量，然后再将特征向量输入到分类器中，分类器自动输出这个文本的类别。其中特征工程是整个机器学习工程问题中的核心问题。这个部分决定着机器学习的性能上限。在很多比赛中，构造特征是最为重要的一步。哪个分类器更适合比赛，这个也是大家应该面临的问题。

## 特征工程
### 文本特征提取
1. 经典的文本特征：经过前人的研究，证明了的优秀的文本特征
2. 手工构造新特征：根据所给的数据，进行相关的特征挖掘和提取
3. 用神经网络提取：用神经网络的某一层输出作为样本的特征

### 经典的文本特征
TF/TFIDF：经常使用  
Doc2Vec/Word2Vec：主要利用深度学习进行训练  
Word2Vec：把一个词转化成一个向量，通常称为词向量。一篇文章是由若干个词组成的，所以我们就可以构造出一篇文章的特征。可以取这篇文章的所有词向量的平均值作为特征，也可以将词向量前后拼接起来，形成长向量，作为这篇文章的特征。

### 构造新特征
1. 寻找可能影响分类的新特征
2. 人工构造可能影响分类的新特征

### 神经网络提取
&emsp;&emsp;我们可以将神经网络作为特征的提取，最左侧是输入层，最右侧是输出层，输入层的输入是文本的所有词向量，然后经过中间层的运算，最后到输出层得到文本对应的类别。采用神经网络相当于提取器加上分类器功能。  
&emsp;&emsp;经验：在文本分类任务上，深度学习和传统机器学习，对于10万数据量级别，传统的方法往往优于深度学习，当文本量级达到100万时，深度学习会优于传统的方法。

### 特征选择
&emsp;&emsp;特征选择解释：从所有特征中提取一些不重要，不相关的特征，从而保留一些主要的特征，减少特征的维数。  
&emsp;&emsp;为什么要进行特征选择呢？特征维数过大，我们的计算机性能不够，运行一次程序的时间太长。特征过多，模型的维度就会变大，模型变复杂，学习会更加困难。  
&emsp;&emsp;特征选择的方法有哪些？主要有三种：包裹式、嵌入式、过滤式。《机器学习》西瓜书的特征 中有介绍。
- 包裹式：随机地从所有特征中抽取出一部分特征来作为文本的特征，然后训练模型，看看模型所能达到最好的性能，再重复上一个过程，重新采集特征训练模型，看看模型能达到的最好性能，不断重复该过程。把模型性能最好的特征子集作为最终要选择的特征。这种方法最大特点就是时间消耗较大。
- 过滤式：在分类器训练之前，单独的考虑每个特征的重要性。举例：可以通过计算一个特征的方差，如果这个特征的方差几乎为零，证明每个样本在这个特征上的值几乎是相同的，所以这个特征就不具备辨识度（即不重要），这样就可以剔除该特征。
- 嵌入式：比如要对TFIDF特征选择，用TFIDF训练一个线性SVM分类器，最终得到一个线性SVM模型，最后得到的是SVM中权重W，权重W越小，与之对应的特征越不重要，所以利用这种方法，根据权重W的大小，选取重要的特征。

### 特征降维
&emsp;&emsp;特征降维解释：将原向量通过数学映射，将其转化为一个维数更低的向量，不同于特征选择，特征选择是在所有特征中选择出一部分重要的特征，而特征降维是将原始特征进行一个数学变换。两者作用是类似的，一方面减少计算量，一方面让模型容易学习。还有一点优势的地方就是在经过特征降维后的向量，往往具有更强的特征表达能力。  
&emsp;&emsp;特征降维主要包括两个方面：有监督降维（LDA），无监督降维（LSA，lda，NMF）  

### 达观杯之article列怎么使用？
&emsp;&emsp;达观提供的数据样本主要有两部分信息：word_seg列（文章由词组成）、article列（文章由字组成）。word_seg列可以用于构建上述所有特征，同理article列也可以用于构建上述所有特征，最后将两者的特征进行融合，形成更强的特征，尽可能的利用有助于分类的特征。  

## 分类器
### 主要分类器
&emsp;&emsp;平常所使用的分类器主要有：基于sklearn实现的Logistic回归、SVM、朴素贝叶斯、随机森林、bagging；Lightgbm（速度更快）；xgboost；神经网络。  
&emsp;&emsp;在面对大规模的稀疏特征时，应该选择Logistic回归，因为其他分类器所需要的运行时间长。所以通常面对机器学习任务时，都会优先用Logistic回归试试，看看效果，心里对这个问题有一个大概估计，然后尝试其他复杂的分类器去提高模型的性能，Lightgbm和xgboost这两种分类器是各大比赛的大杀器。这两个分类器都是基于GBDT算法实现的，是梯度提升算法中的一种。  

### 提分关键：多个单模型进行融合
融合方法：投票法、学习法  
- 投票法：对各个单模型进行统计，选择预测结果数量最多的那个类别，通常称为绝对投票。对每个单模型赋予不同的权重，再进行结果融合。
关键：训练多个好而不同的单模型  
补充：构造多个不同的训练集，对整个数据集进行随机采样。

## 比赛代码
github地址：https://github.com/MLjian/TextClassificationImplement